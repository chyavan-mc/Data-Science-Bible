{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In predictive modeling, the goal is to come up with a machine learning model to predict response variable $Y$ given the independent variable $X$. Here, both $X$ and $Y$ can be multi-dimensional. Mathematically, the independent variable may be related to the response in the form of - \n",
    "\n",
    "$$ Y = f(X) + \\epsilon $$\n",
    "\n",
    "We may be interested in predicting $f$ for **Prediction**, **Inference**, or a combination of both. We can denote the estimate of the function $f$ as $\\hat{f}$. Therefore, when we make a prediction of the response using the function, our prediction is denoted by - \n",
    "\n",
    "$$ \\hat{Y} = \\hat{f}(X) $$\n",
    "\n",
    "* **Regression** - Problems where the response variable is quantitative\n",
    "* **Classification** - Problems where the response variable is qualitative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error in the estimate\n",
    "\n",
    "We can mathematically define and divide the error in the estimate as follows - \n",
    "\n",
    "$$ \\begin{align*} E[Y-\\hat{Y}]^2 &= E[f(X) + \\epsilon - \\hat{f}(X)]^2 \\\\  &= [f(X) - \\hat{f}(X)]^2 + Var(\\epsilon) \\end{align*} $$\n",
    "\n",
    "Here, the first term represents the *reducible* error and the second term represents the *irreducible error*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric and Non-Parametric Models\n",
    "\n",
    "To estimate $f$ we can choose a parametric or non-parametric approach.\n",
    "\n",
    "1. Parametric Models - In this approach, we make an assumption about the functional form of $f$, therefore reducing the problem of estimating $f$ down to one of estimatic a set of parameters\n",
    "    * The assumption of the functional form of $f$ introduces 'model bias'.\n",
    "    * We only have a few parameters to finetune in this approach\n",
    "2. Non-parametric Models - In this approach, we do not make an explicit assumption about the functional form of $f$. Instead, we try to estimate $f$ that can get as close to the data points as possible\n",
    "    * Since the problem is not simply estimating a small number of parameters, a very large number of observations are required for an accurate estimation of f\n",
    "    * We have more flexibility introducing more options for finetuning the model known as hyper-parameters\n",
    "\n",
    "#### Accuracy vs. Interpretability\n",
    "\n",
    "As the complexity of the model increases -\n",
    "* The accuracy of the predictions may increase\n",
    "* The interpretability of the model may decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias Variance Trade-off\n",
    "\n",
    "* **Variance** refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set.\n",
    "    * As the complexity of the model increases, the variance of the model increases\n",
    "    * Error due to model variance can be reduced by considering more observations\n",
    "* **Bias** refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much\n",
    "simpler mathematical model.\n",
    "    * As the complexity of the model increases, the bias of the model decreases\n",
    "    * Error due to model bias can be reduced by increasing the complexity of the model (Also increases variance)\n",
    "\n",
    "When measuring the error on the test set, we can write the *expected test MSE* as follows -\n",
    "\n",
    "$$\n",
    "E_{\\mathit{D}}[\\left( y(x|\\mathit{D}) - h(x) \\right)^2] = [E_{\\mathit{D}}[y(x|\\mathit{D})] - h(x)]^2 + E_{\\mathit{D}}[\\left( y(x|\\mathit{D}) - E_{\\mathit{D}}[y(x|\\mathit{D})] \\right)^2] + Var(\\epsilon)\n",
    "$$\n",
    "\n",
    "Here,\n",
    "* $\\mathit{D}$ represents the dataset under consideration\n",
    "* $y(x|D)$ represents the estimated response of the model given the dataset the model is trained on\n",
    "* $h(x)$ represents the true function's response (without the irreducible error) for the given x\n",
    "\n",
    "<br>\n",
    "\n",
    "* The first term corresponds to $(bias)^2$ when the expectation is evaluated by integrating over the whole dataset $\\mathit{D}$. It simply measures how good the average model is.\n",
    "* The second term corresponds to the model $variance$ when the expectation is evaluated by integrating over the whole dataset $\\mathit{D}$. It simply measures how sensitive the model is to variations in the dataset.\n",
    "* The third term is the irreducible error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Setting\n",
    "\n",
    "One of the metrics to evaluate the classification setting is the *error rate*. It is the proportion of mistakes that are made if we apply error rate our estimate $\\hat{f}$ to the observations. Mathematically the error rate is defined as -\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^{n} I(y_i \\neq \\hat{y}_i ) $$\n",
    "\n",
    "The **Bayes classifier** assigns each observation to the most likely class, given its predictor values. It uses the conditional distribution of $Y$ given $X$. It basically finds the class label $j$ for which the following quantity is maximum.\n",
    "\n",
    "$$P(Y=j | X=x_0)$$\n",
    "\n",
    "The Bayesian decision boundary is the boundary in the $X$ space, where the probabilities of multiple classes (that are the majority on either side of the boundary) are equal. The error rate in Bayes Classifier is given as follows -\n",
    "\n",
    "$$ 1 - E \\left[ max_j P(Y = j|X) \\right] $$\n",
    "\n",
    "This considers the maximum probability (*expected value* over the complete dataset) associated with any class for each prediction, and the error is how much it deviates from 1 (1 representing total confidence in classifying the observation to a particular class). The Bayes error rate exists because of the irreducible error that moves a class away from the ideal boundary and into the opposite class's region?\n",
    "\n",
    "Bayes classifier is the most ideal classifier as it predicts the most likely class based on the conditional probability distribution. But, in reality, this probability distribution is hard to model and hence Bayes Classifier is not a practical model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior vs Likelihood Estimation\n",
    "\n",
    "Bayes Theorem is mathematically given by - \n",
    "\n",
    "$$ P(Y|X) = \\frac{P(Y \\cap X)}{P(X)} = \\frac{P(X|Y).P(Y)}{P(X)} $$\n",
    "\n",
    "Here,\n",
    "* $P(Y|X)$ is known as the *posterior probability*\n",
    "* $P(X|Y)$ is known as the *likelihood*\n",
    "* $P(Y)$ is known as the *prior probability*\n",
    "* $P(X)$ is known as the *marginal probability*. This term does not have an impact on the prediction of $Y$\n",
    "\n",
    "Some statistical methods model the likelihood whereas some model directly model the posterior probability. When modeling likelihood, we need to be aware of the prior as an imbalanced classifier with a large difference in the prior probability of the classes.\n",
    "\n",
    "Likelihood based models\n",
    "* Logistic Regression\n",
    "* Naive Bayes\n",
    "* K-Nearest Neighbors\n",
    "* Decision trees\n",
    "* Neural Networks - By default, using cross entropy loss in neural network follows Maximum Likelihood Estimation (MLE) and not Maximum A Posteriori (MAP)\n",
    "\n",
    "Posterior based models\n",
    "* Random Forest\n",
    "* Bayesian Neural Networks\n",
    "\n",
    "> **Doubtful - Need to clarify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "**K-NN Regression**\n",
    "In the regression setting, given an input $X$, we find the *K* nearest neighbors of the observation and average the response $Y$ for the K training data points. This is a relaxation of the method to estimate the expectation given we have the distribution of the response for the given X. Mathematically -\n",
    "\n",
    "$$ \\hat{f}(x_0) = \\frac{1}{K} \\sum_{i \\in \\mathit{N_o}} y_i $$\n",
    "\n",
    "**K-NN Classification**\n",
    "In the classification setting, given an input $X$, we find the *K* nearest neighbors ($N_o$) of the observation and provide the probabilities of each class as the fraction of the data points out of the K neighbors that belong to that class. Mathematically -\n",
    "\n",
    "$$ P(Y=j | X=x_0) = \\frac{1}{K} \\sum_{i \\in \\mathit{N_o}} I(y_i = j) $$\n",
    "\n",
    "**Disadvantages of K-NN**\n",
    "* K-NN memorizes the whole dataset and hence would require a large memory and at the same time is susceptible to attacks ([Membership Inference Attacks](https://arxiv.org/abs/1610.05820))\n",
    "* **Curse of Dimensionality** - As the number of dimensions increase, the radius of the hyper-sphere increases (as compared to the bounding hyper-box of the dataset). For 10% neighbors, the radius excedes the boundary of the box as the dimensions increase beyond 5.\n",
    "\n",
    "![KNN COD](./assets/ISLR/curse-of-dimensionality.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
