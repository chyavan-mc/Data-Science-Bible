{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a linear regression model, we intend to approach problems where the response variable is quantitative. The relationship is expressed in the form of an equation or a model connecting the response or dependent variable and one or more explanatory or predictor variable. The general mathematical equation for linear regression is given by -\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Y &= f(X_1, X_2, ..., X_p) + \\epsilon \\\\\n",
    "Y &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reasonable approach to fit a line (find the $\\beta$ values) is to minimize the amount by which the fitted value differs from the actual value. An assumption in this approach is that the predictor variables are observed without any uncertainty but the response variable may have variance due to irreducible error. This leads to the *Ordinary Least Squares* method.\n",
    "\n",
    "Alternate fitting method: Another approach for the fit is to assume that both the independent and dependent variables have uncertainty in measurement. This leads to the *Total Least Squares* method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinary Least Squares** <br>\n",
    "This approach is an MLE solution for obtaining the best regression line. This method minimizes the *Residual Sum of Squares (RSS)* or the *Mean Squared Error (MSE)*\n",
    "\n",
    "$$ RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Therefore the objective of the approach is given as,\n",
    "\n",
    "$$\n",
    "\\min_{\\beta_0, ..., \\beta_p} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\: \\text{, or} \\\\\n",
    "\\\\\n",
    "\\min_{\\beta_0, ..., \\beta_p} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - [\\hat{\\beta_0} + \\hat{\\beta_1} x_1 + \\hat{\\beta_2} x_2 + ... + \\hat{\\beta_p} x_p])^2 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, for simple linear regression the $\\beta$ values are given as follows.\n",
    "\n",
    "$$\\hat{\\beta}_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = r_{xy} \\frac{\\sigma_y}{\\sigma_x}$$\n",
    "\n",
    "$$\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta_1}\\bar{x}$$\n",
    "\n",
    "Note:\n",
    "* We can see that when $\\hat{\\beta}_1 = 0$, then the $\\hat{\\beta}_0$ estimate is just the mean of the response variable and this results in a model called the *null model*\n",
    "* Here $r_{xy}$ represents the correlation between x and y for the data (sample). Pearson correlation uses $\\rho_{xy}$ for population and $r_{xy}$ for samples.\n",
    "* When using the lag of response itself as the predictor (Autoregression), we can see that the $\\hat{\\beta}_1$ estimate reduces to just the $r_{xy}$ as the variance of $x$ and $y$ are (approximately) the same. Therefore, in autoregression, the beta corresponds to the value of auto-correlation between the response and the lagged response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_diabetes()\n",
    "\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "X = sm.add_constant(X[['age', 'sex', 'bmi', 'bp']])\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   72.91</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 20 Jul 2023</td> <th>  Prob (F-statistic):</th> <td>2.70e-47</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>03:56:31</td>     <th>  Log-Likelihood:    </th> <td> -2434.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   442</td>      <th>  AIC:               </th> <td>   4878.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   437</td>      <th>  BIC:               </th> <td>   4899.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  152.1335</td> <td>    2.853</td> <td>   53.329</td> <td> 0.000</td> <td>  146.527</td> <td>  157.740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>   <td>   37.2406</td> <td>   64.117</td> <td>    0.581</td> <td> 0.562</td> <td>  -88.776</td> <td>  163.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex</th>   <td> -106.5762</td> <td>   62.125</td> <td>   -1.716</td> <td> 0.087</td> <td> -228.677</td> <td>   15.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bmi</th>   <td>  787.1817</td> <td>   65.424</td> <td>   12.032</td> <td> 0.000</td> <td>  658.597</td> <td>  915.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bp</th>    <td>  416.6725</td> <td>   69.495</td> <td>    5.996</td> <td> 0.000</td> <td>  280.087</td> <td>  553.258</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 9.858</td> <th>  Durbin-Watson:     </th> <td>   1.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.007</td> <th>  Jarque-Bera (JB):  </th> <td>   6.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.146</td> <th>  Prob(JB):          </th> <td>  0.0395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.485</td> <th>  Cond. No.          </th> <td>    28.4</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.400\n",
       "Model:                            OLS   Adj. R-squared:                  0.395\n",
       "Method:                 Least Squares   F-statistic:                     72.91\n",
       "Date:                Thu, 20 Jul 2023   Prob (F-statistic):           2.70e-47\n",
       "Time:                        03:56:31   Log-Likelihood:                -2434.2\n",
       "No. Observations:                 442   AIC:                             4878.\n",
       "Df Residuals:                     437   BIC:                             4899.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        152.1335      2.853     53.329      0.000     146.527     157.740\n",
       "age           37.2406     64.117      0.581      0.562     -88.776     163.258\n",
       "sex         -106.5762     62.125     -1.716      0.087    -228.677      15.525\n",
       "bmi          787.1817     65.424     12.032      0.000     658.597     915.766\n",
       "bp           416.6725     69.495      5.996      0.000     280.087     553.258\n",
       "==============================================================================\n",
       "Omnibus:                        9.858   Durbin-Watson:                   1.933\n",
       "Prob(Omnibus):                  0.007   Jarque-Bera (JB):                6.464\n",
       "Skew:                           0.146   Prob(JB):                       0.0395\n",
       "Kurtosis:                       2.485   Cond. No.                         28.4\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "display(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients\n",
    "\n",
    "Assume the above example. Considering the independent variables `age`, `sex`, `bmi`, `bp` to predict the `disease progression` as a regression response. Mathematically - \n",
    "\n",
    "$$ \\hat{Prog} = \\hat{\\beta}_0 + \\hat{\\beta}_1.age + \\hat{\\beta}_2.sex + \\hat{\\beta}_3.bmi + \\hat{\\beta}_4.bp + \\epsilon $$\n",
    "\n",
    "\n",
    "**Interpretation** <br>\n",
    "Each coefficient can be interpreted as the marginal effect on the response variable with respect to the independent variable considering all other independent variables are held constant. Mathematically,\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = \\beta_i$$\n",
    "\n",
    "We can generalize the interpretation of a coefficients in relation to the marginal effect using the following table.\n",
    "\n",
    "\n",
    "| Equation | Marginal Effect |\n",
    "|---|---|\n",
    "| $y = b_0 + b_1 x$ | $\\frac{\\partial y}{\\partial x} = b_1$ |\n",
    "| $y = b_0 + b_1 ln(x)$ | $\\frac{\\partial y}{\\partial x} = \\frac{b_1}{x}$ |\n",
    "| $ln(y) = b_0 + b_1 x$ | $\\frac{\\partial y}{\\partial x} = b_1 y$ |\n",
    "| $ln(y) = b_0 + b_1 ln(x)$ | $\\frac{\\partial y}{\\partial x} = b_1 \\frac{y}{x}$ |\n",
    "\n",
    "\n",
    "**Standard Error** <br>\n",
    "Here, since we're considering only a sample of the population as our dataset, the $\\beta$ etimates can be an over-estimate or under-estimate. But the OLS method produces an unbiased estimate of the $\\beta$, meaning the average over a large number of estimates converges to the true value of the $\\beta$. Therefore, for the dataset (sample), we can estimate the *Standard Error (SE)* similar to estimating the SE of the mean of a dataset.\n",
    "\n",
    "In our example, the SE for the estimate of the coefficient for `age` ($\\hat{\\beta}_1$) is $64.117$.\n",
    "\n",
    "**T-Statistic** <br>\n",
    "This value by itself is not useful unless we can compare it with the magnitude of the coefficient estimate. For age, we have $\\hat{\\beta}_1 = 37.2406$ which means that the coefficient may differ from the true value by a value of $64.117$ on average. We can perform hypothesis testing to make sure the coefficient is not zero.\n",
    "* $H_0$ - There is no relationship between $X$ and $Y$, i.e. $\\beta = 0$\n",
    "* $H_a$ - There is some relationship between $X$ and $Y$, i.e. $\\beta \\neq 0$\n",
    "\n",
    "The t-statistic is calculated for a degree of freedom of $n-2$. Mathematically,\n",
    "\n",
    "$$t = \\frac{\\hat{\\beta}-0}{SE(\\hat{\\beta})}$$\n",
    "\n",
    "**P-value** <br>\n",
    "Given the t-statistic, we can find the p-value as the probability of observing a value $|t|$ or larger given $\\beta = 0$. In other words, the p-value is a probability value that quantifies the likelihood of obtaining the observed results (or more extreme) under the assumption that the null hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Assessment\n",
    "\n",
    "**Residual Standard Error (RSE)** <br>\n",
    "This is the estimate of the quantity $\\sigma$ where $\\sigma^2$ is then variance of $\\epsilon$. The RSE is calculated as - \n",
    "\n",
    "$$RSE = \\sqrt{\\frac{1}{n-p-1} RSS} = \\sqrt{\\frac{1}{n-p-1} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}$$\n",
    "\n",
    "RSE is measured in the same units as the response variable, and the value of RSE roughly corresponds to the average amount the response will deviate from the true regression line.\n",
    "\n",
    "Note: Root Mean Squared Error (RMSE) is a similar measure, the difference being the mean considers $n$ in the denominator instead of the *degrees of freedom* of the residuals.\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}$$\n",
    "\n",
    "\n",
    "**R-squared** <br>\n",
    "R-squared measures the proportion of variability in $Y$ that can be explained using $X$. Mathematically - \n",
    "\n",
    "$$R^2 = \\frac{TSS-RSS}{TSS} = 1 - \\frac{RSS}{TSS} $$\n",
    "\n",
    "where,\n",
    "* $RSS = \\sum (y_i - \\hat{y}_i)^2$. RSS represents the sum of squares of the residuals of the regression model.\n",
    "* $TSS = \\sum (y_i - \\bar{y})^2$. TSS represents teh sum of squares of the residuals of the null model.\n",
    "\n",
    "NOTE: For simple linear regression (including Autoregression with one predictor), the $R^2$ statistic is the equal to the squared correlation between the response and the predictor, $r^2$\n",
    "\n",
    "\n",
    "**F-Statistic** <br>\n",
    "F-statistic is a metric used to perform hypothesis testing to whether all the coefficients are 0. \n",
    "* $H_0$ - $\\beta_1 = \\beta_2 = ... = \\beta_p = 0$\n",
    "* $H_a$ - At least one $\\beta_i$ is non-zero\n",
    "\n",
    "$$F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)}$$\n",
    "\n",
    "F-statistic can easily be converted to a p-value to understand more easily the significance of the F-statistic's value given the degrees of freedoms for the statistic. Use the F-distribution with $df_1 = p$ and $df_2 = n-p-1$ to determine the p-value.\n",
    "\n",
    "NOTE: It is important to evaluate the F-statistic and the overall p-value. Consider a large number of independent variables, say $p=100$. If we have a significance level of 0.05, 5 variables may have a significant p-value just by chance. However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors.\n",
    "\n",
    "> AIC, BIC, Adjusted R<sup>2</sup> - can be used for model/variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection - Classical Approaches\n",
    "\n",
    "The following variable selection methods can be used when it is cumbersome to build $2^p$ models and evaluate each model to choose the best subset of variables for the linear regression model.\n",
    "\n",
    "1. **Forward selection** - Start with the null model, and perform a greedy process of adding one predictor at a time (with the lowest p-value). This procedure is repeated until a stopping criteria is reached.\n",
    "\n",
    "2. **Backward selection** - Start with all variables in the model, and perform a greedy process of removing one predictor at a time (with the highest p-value). This procedure is repeated until a stopping criteria is reached.\n",
    "\n",
    "3. **Mixed selection** - This is a combination of forward and backward selection. Start with no variables in the model and follow forward selection. When a variable crosses a significance threshold in the p-value, start removing the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "1. Confidence interval: Reducible Error -->  how close $\\hat{Y}$ will be to $f(X)$\n",
    "2. Prediction interval: Irreducible Error + Reducible Error -->  how close $\\hat{Y}$ will be to $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interaction terms\n",
    "> \n",
    "> Power terms\n",
    "> \n",
    "> Potential Problems\n",
    "> 1. Non-linearity of the response-predictor relationships\n",
    "> 2. Correlation of error terms\n",
    "> 3. Non-constant variance of error terms\n",
    "> 4. Outliers\n",
    "> 5. High-leverage points\n",
    "> 6. Collinearity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
